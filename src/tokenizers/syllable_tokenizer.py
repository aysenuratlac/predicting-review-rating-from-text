"""
Turkish Syllable Tokenizer
Uses turkishnlp library for syllabication
"""

from turkishnlp import detector
import re

class TurkishSyllableTokenizer:
    """
    TÃ¼rkÃ§e hece tokenizer - sklearn ile uyumlu
    
    turkishnlp kÃ¼tÃ¼phanesini kullanarak metni hecelere ayÄ±rÄ±r.
    
    Usage:
        tokenizer = TurkishSyllableTokenizer()
        syllables = tokenizer.tokenize("Harika bir restoran")
        # ['ha', 'ri', 'ka', 'bir', 'res', 'to', 'ran']
        
        # sklearn ile kullanÄ±m:
        from sklearn.feature_extraction.text import CountVectorizer
        vectorizer = CountVectorizer(tokenizer=tokenizer.tokenize)
    """
    
    def __init__(self, lowercase=True, remove_punctuation=True):
        """
        Parameters:
        -----------
        lowercase : bool, default=True
            Metni kÃ¼Ã§Ã¼k harfe Ã§evir
        remove_punctuation : bool, default=True
            Noktalama iÅŸaretlerini kaldÄ±r
        """
        self.lowercase = lowercase
        self.remove_punctuation = remove_punctuation
        
        # turkishnlp objesi oluÅŸtur
        self.nlp = detector.TurkishNLP()
        
        # Veri setlerini oluÅŸtur (ilk kulanÄ±mda gerekli)
        try:
            self.nlp.create_word_set()
        except:
            pass  # Zaten oluÅŸturulmuÅŸsa sorun yok
        
        print("âœ… TurkishSyllableTokenizer hazÄ±r!")
    
    def tokenize(self, text):
        """
        Metni hecelere ayÄ±r
        
        Parameters:
        -----------
        text : str
            Tokenize edilecek metin
            
        Returns:
        --------
        list of str
            Heceler listesi
        """
        if not text or not isinstance(text, str):
            return []
        
        # Preprocessing
        if self.lowercase:
            text = text.lower()
        
        if self.remove_punctuation:
            # Noktalama iÅŸaretlerini kaldÄ±r
            text = re.sub(r'[^\w\s]', '', text)
        
        # BoÅŸluklara gÃ¶re kelimelere ayÄ±r
        words = text.split()
        
        # Her kelimeyi hecelere ayÄ±r
        all_syllables = []
        for word in words:
            if word.strip():  # BoÅŸ deÄŸilse
                try:
                    syllables = self.nlp.syllabicate(word)
                    all_syllables.extend(syllables)
                except Exception as e:
                    # Hata olursa kelimeyi olduÄŸu gibi ekle
                    all_syllables.append(word)
        
        return all_syllables
    
    def __call__(self, text):
        """
        sklearn uyumluluÄŸu iÃ§in
        """
        return self.tokenize(text)


def turkish_syllable_tokenizer(text):
    """
    Basit fonksiyon wrapper (sklearn iÃ§in)
    
    NOT: Her Ã§aÄŸrÄ±da yeni obje oluÅŸturmaz, bu yÃ¼zden daha verimli.
    Ama ilk kullanÄ±mda obje oluÅŸturma maliyeti var.
    """
    # Global obje (ilk kullanÄ±mda oluÅŸtur)
    if not hasattr(turkish_syllable_tokenizer, '_tokenizer'):
        turkish_syllable_tokenizer._tokenizer = TurkishSyllableTokenizer()
    
    return turkish_syllable_tokenizer._tokenizer.tokenize(text)


# Test
if __name__ == "__main__":
    print("ğŸ§ª Hece Tokenizer Test\n")
    
    # Tokenizer oluÅŸtur
    tokenizer = TurkishSyllableTokenizer()
    
    # Test metinleri
    test_texts = [
        "Harika bir restoran!",
        "Ã‡ok gÃ¼zel ve lezzetli yemekler.",
        "Istanbul'un en iyi mekanÄ±.",
        "Berbat bir deneyim."
    ]
    
    print("Test metinleri:\n")
    for text in test_texts:
        syllables = tokenizer.tokenize(text)
        print(f"  Metin: {text}")
        print(f"  Heceler: {syllables}")
        print(f"  Toplam: {len(syllables)} hece\n")
    
