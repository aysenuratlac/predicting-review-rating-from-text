"""
Turkish Word Tokenizer
NLTK word_tokenize kullanarak
"""

import re
import nltk
from nltk.tokenize import word_tokenize

class TurkishWordTokenizer:
    """
    T√ºrk√ße kelime tokenizer - sklearn ile uyumlu
    
    NLTK word_tokenize kullanƒ±r, T√ºrk√ße dil desteƒüi ile.
    
    Usage:
        tokenizer = TurkishWordTokenizer()
        words = tokenizer.tokenize("Harika bir restoran!")
        # ['harika', 'bir', 'restoran']
        
        # sklearn ile kullanƒ±m:
        from sklearn.feature_extraction.text import CountVectorizer
        vectorizer = CountVectorizer(tokenizer=tokenizer.tokenize)
    """
    
    def __init__(self, lowercase=True, remove_punctuation=True, remove_stopwords=False):
        """
        Parameters:
        -----------
        lowercase : bool, default=True
            Metni k√º√ß√ºk harfe √ßevir
        remove_punctuation : bool, default=True
            Noktalama i≈üaretlerini kaldƒ±r
        remove_stopwords : bool, default=False
            T√ºrk√ße stopword'leri kaldƒ±r
        """
        self.lowercase = lowercase
        self.remove_punctuation = remove_punctuation
        self.remove_stopwords = remove_stopwords
        
        # NLTK punkt tokenizer kontrol
        try:
            nltk.data.find('tokenizers/punkt')
        except LookupError:
            print("‚ö†Ô∏è  NLTK punkt tokenizer indiriliyor...")
            nltk.download('punkt', quiet=True)
        
        # T√ºrk√ße stopwords
        self.turkish_stopwords = set([
            've', 'veya', 'ama', 'fakat', 'ancak', 'lakin',
            'i√ßin', 'ile', 'da', 'de', 'ki', 'mi', 'mƒ±', 'mu', 'm√º',
            'bir', 'bu', '≈üu', 'o', 'her', 'bazƒ±', '√ßok', 'az',
            'ne', 'nasƒ±l', 'neden', 'ni√ßin', 'nerede', 'kim', 'ben', 'sen',
            'biz', 'siz', 'onlar', '≈üey', 'gibi', 'kadar', 'daha',
            'en', 'pek', 'olduk√ßa', 'son', 'ilk', 'var', 'yok', 'olan'
        ])
        
        print(f"‚úÖ TurkishWordTokenizer hazƒ±r! (NLTK)")
    
    def tokenize(self, text):
        """
        Metni kelimelere ayƒ±r
        
        Parameters:
        -----------
        text : str
            Tokenize edilecek metin
            
        Returns:
        --------
        list of str
            Kelimeler listesi
        """
        if not text or not isinstance(text, str):
            return []
        
        # NLTK ile tokenize (T√ºrk√ße dil desteƒüi)
        tokens = word_tokenize(text, language='turkish')
        
        # Lowercase
        if self.lowercase:
            tokens = [t.lower() for t in tokens]
        
        # Noktalama kaldƒ±r
        if self.remove_punctuation:
            # Sadece harf/rakam i√ßeren token'larƒ± tut
            tokens = [t for t in tokens if re.search(r'\w', t)]
        
        # Stopword kaldƒ±r
        if self.remove_stopwords:
            tokens = [t for t in tokens if t not in self.turkish_stopwords]
        
        return tokens
    
    def __call__(self, text):
        """
        sklearn uyumluluƒüu i√ßin
        """
        return self.tokenize(text)


def turkish_word_tokenizer(text):
    """
    Basit fonksiyon wrapper (sklearn i√ßin)
    """
    # Global obje (ilk kullanƒ±mda olu≈ütur)
    if not hasattr(turkish_word_tokenizer, '_tokenizer'):
        turkish_word_tokenizer._tokenizer = TurkishWordTokenizer()
    
    return turkish_word_tokenizer._tokenizer.tokenize(text)


# Test
if __name__ == "__main__":
    print("üß™ Kelime Tokenizer Test\n")
    
    # Tokenizer olu≈ütur
    tokenizer = TurkishWordTokenizer(
        lowercase=True,
        remove_punctuation=True,
        remove_stopwords=False
    )
    
    # Test metinleri
    test_texts = [
        "Harika bir restoran! √áok beƒüendim.",
        "G√ºzel mekan ama yemekler soƒüuktu.",
        "Istanbul'un en iyi yerlerinden biri.",
        "5/5 puan veriyorum, m√ºkemmel hizmet."
    ]
    
    print("Test metinleri:\n")
    for text in test_texts:
        tokens = tokenizer.tokenize(text)
        print(f"  Metin: {text}")
        print(f"  Tokenlar: {tokens}")
        print(f"  Toplam: {len(tokens)} token\n")
    
    # Stopword ile test
    print("\n" + "="*50)
    print("Stopword kaldƒ±rma ile test:\n")
    tokenizer_no_stop = TurkishWordTokenizer(
        lowercase=True,
        remove_punctuation=True,
        remove_stopwords=True
    )
    
    sample = "Bu bir harika restoran ve √ßok g√ºzel"
    print(f"Metin: {sample}")
    print(f"Normal: {tokenizer.tokenize(sample)}")
    print(f"Stopword'sƒ±z: {tokenizer_no_stop.tokenize(sample)}")
    
